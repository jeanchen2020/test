!pip install jieba




#--------------------------------
# 匯入外部模組
#--------------------------------
import jieba

#--------------------------------
# 存停用詞, 分詞, 過濾後分詞的list
#--------------------------------
stopWords=[]
segments=[]
remainderWords=[]

#--------------------------------
# 讀入停用詞檔
#--------------------------------
with open('stopWords.txt', 'r', encoding='UTF-8') as file:
    for data in file.readlines():
        data = data.strip()
        stopWords.append(data)

#--------------------------------
# 讀入文件檔, 進行中文斷詞
#--------------------------------
with open('20201022.txt', 'r', encoding='UTF-8') as file:
    #讀入文檔
    text = file.read()

    #結巴中文斷詞
    segments = jieba.cut(text, cut_all=False)

#------------------------------
# 移除停用詞及跳行符號
#------------------------------
remainderWords = list(filter(lambda a: a not in stopWords and a != '\n', segments))

#------------------------------
# 印出過濾後的分詞
#------------------------------
for k in remainderWords:
    print(k)
    
    
    
    
    
import jieba.analyse
print('textrank:')
for x, w in jieba.analyse.textrank(text, withWeight=True):
    print('%s %s' % (x, w))
  
  
  
  
tags = jieba.analyse.extract_tags(text,withWeight=True)
for tag in tags:
    print('word:', tag[0], 'tf-idf:', tag[1])
    
    
    
    
!pip install collections
!pip install wordcloud
!pip install matplotlib
from collections import Counter
from wordcloud import WordCloud
from matplotlib import pyplot as plt
